{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPuQEjWSrtDW2N//D4XpzrY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arguto1993/wordcloud-berita-pusdiklat-bps-2023/blob/main/WordCloud_Berita2600.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ],
      "metadata": {
        "id": "Z2HDY04XX4d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wordcloud"
      ],
      "metadata": {
        "id": "_XCkHmMjzEcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ecad2d-9092-46ad-f8e5-ea370cd1592a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "s45sKt8zaLwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load stopwords"
      ],
      "metadata": {
        "id": "v6ckJSNuXtoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load stopwords"
      ],
      "metadata": {
        "id": "KWHxjxhdX0d_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load STOPWORDS from nltk library"
      ],
      "metadata": {
        "id": "iTUp5qsIjtXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk  # Example using NLTK\n",
        "nltk.download('stopwords')  # Download if necessary\n",
        "STOPWORDS = set(nltk.corpus.stopwords.words('english'))  # Load English stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY2i1RC0jVoc",
        "outputId": "dcec7651-14d5-416e-acaf-1394008f33ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load stopwords from file (need to upload the file manually to colab files)"
      ],
      "metadata": {
        "id": "E4jRp_lrYExy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Indonesian stopwords from the file\n",
        "# with open(\"stopwords-id.txt\", \"r\") as f:\n",
        "#     stopwords_id = f.read().splitlines()\n",
        "\n",
        "# Load Custom stopwords from the file\n",
        "# with open(\"stopwords-p.txt\", \"r\") as f:\n",
        "#     stopwords_p = f.read().splitlines()"
      ],
      "metadata": {
        "id": "1pcBx8u4YRUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load stopwords from google drive (recommended)"
      ],
      "metadata": {
        "id": "uWAuMysgYWX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "Hl5S1v60ociv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file_from_drive(file_id, filename):\n",
        "    \"\"\"Downloads a file from Google Drive using its ID and saves it to a specified filename.\"\"\"\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}&export=download\"\n",
        "    response = requests.get(url, stream=True)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                f.write(chunk)\n",
        "            print(f\"Success: {filename} downloaded from Google Drive.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Error: Failed to download {filename} from Google Drive.\")\n",
        "        return False\n",
        "\n",
        "# Download stopwords-id.txt\n",
        "if download_file_from_drive(\"1EyhKHCCAJOHBA_iicz4ry-VLcu3RmXZE\", \"stopwords-id.txt\"):\n",
        "    with open(\"stopwords-id.txt\", \"r\") as f:\n",
        "        stopwords_id = f.read().splitlines()\n",
        "\n",
        "# Download stopwords-p.txt\n",
        "if download_file_from_drive(\"12ZjGx05K3FfSeKPyMYhhalrldZHR6Zaz\", \"stopwords-p.txt\"):\n",
        "    with open(\"stopwords-p.txt\", \"r\") as f:\n",
        "        stopwords_p = f.read().splitlines()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpRcOwGXYcxF",
        "outputId": "12e8d6df-34fa-4956-b23f-ec8dd77a3c22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine stopwords"
      ],
      "metadata": {
        "id": "X9slIoR8YfWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_udkBwOFB-C4"
      },
      "outputs": [],
      "source": [
        "# Combine default and custom stopwords\n",
        "stopwords_c = set(STOPWORDS).union(stopwords_id).union(stopwords_p)  # Use a set for efficient membership testing\n",
        "# stopwords_c = stopwords_p"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load clean dataset (cleaned via [spreadsheets](https://docs.google.com/spreadsheets/d/13hsXm20S6RdAyK520rLLkbbUJEywRp5F/edit?usp=sharing&ouid=101051388151123974532&rtpof=true&sd=true))"
      ],
      "metadata": {
        "id": "WKdydwYoZg-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google drive file id for clean dataset\n",
        "file_id = \"13hsXm20S6RdAyK520rLLkbbUJEywRp5F\"\n",
        "# Construct the download URL\n",
        "GSHEET_URL_XLSX = f\"https://drive.google.com/uc?id={file_id}&export=download\"\n",
        "\n",
        "# Download the file using requests\n",
        "response = requests.get(GSHEET_URL_XLSX, stream=True)\n",
        "\n",
        "with open(\"berita2600konten.xlsx\", 'wb') as f:\n",
        "    try:\n",
        "        # for chunk in response.iter_content(chunk_size=2048):\n",
        "        for chunk in response.iter_content(chunk_size=4048):\n",
        "            f.write(chunk)\n",
        "        print(\"Data download and writing complete!\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred during download:\", e)\n",
        "        # Implement error-handling logic here (e.g., retrying, logging)\n",
        "\n",
        "# Specify the engine based on the file format\n",
        "engine = 'openpyxl'  # For .xlsx files\n",
        "# engine = 'xlrd'  # For older .xls files\n",
        "\n",
        "# Read the Excel file using the specified engine\n",
        "data = pd.read_excel(\"berita2600konten.xlsx\", sheet_name=\"data_c3\", engine=engine)"
      ],
      "metadata": {
        "id": "DGDxZIvoZvvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dea8a3e-4ec1-4754-bb16-396fc8d345c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_colwidth = 2500  # Set a larger width for full display\n",
        "print(data[\"konten_clean\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bZx4EbXybb",
        "outputId": "c134b152-5d22-4d44-e9aa-f2db566d444f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean text (adjust as needed)\n",
        "clean_text = (\n",
        "    data[\"konten_clean\"]\n",
        "        # .str.lower()\n",
        "        .str.replace(r\"\\n\", \" \")  # Remove newlines\n",
        "        .str.replace(r\"<[^>]*>\", \" \")  # Remove HTML tags\n",
        "        .str.replace(r\"\\/.,\", \" \")  # Remove punctuation (adjust as needed)\n",
        ")  # Apply cleaning to each row\n",
        "\n",
        "# Tokenize text\n",
        "words_per_row = clean_text.apply(word_tokenize)  # Tokenize each row\n",
        "\n",
        "# Count words in each row and sum across the DataFrame\n",
        "num_words = words_per_row.apply(len).sum()\n",
        "print(\"Number of words (using nltk):\", num_words)\n",
        "\n",
        "# List of words\n",
        "list_words = words_per_row.sum()  # Sum the word counts in each row\n",
        "print(\"List of words (using nltk):\", list_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCMFlTR7c_ph",
        "outputId": "9e4380b9-3802-4287-b7a6-2c8c1c9961e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Word Cloud"
      ],
      "metadata": {
        "id": "lWVClV9eqAsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "from PIL import Image\n",
        "\n",
        "# Function to set text color to white\n",
        "def white_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "    return \"white\"\n",
        "\n",
        "# Load PNG image as a mask\n",
        "# mask = np.array(Image.open(\"shape.png\"))\n",
        "\n",
        "# clean_text_string = data[\"konten_clean\"].str.cat()\n",
        "clean_text_string = data[\"konten_clean\"].str.cat(sep=\" \")\n",
        "\n",
        "# Tokenize the text using nltk\n",
        "words = word_tokenize(clean_text_string)\n",
        "\n",
        "# Join the tokenized words into a string\n",
        "word_string = \" \".join(words)\n",
        "\n",
        "# Generate word cloud with nltk-tokenized text\n",
        "wordcloud = WordCloud(\n",
        "    background_color=\"black\",\n",
        "    max_words=4000,\n",
        "    stopwords=stopwords_c,\n",
        "    width=1600,\n",
        "    height=900,\n",
        "    contour_width=5,  # This should work now\n",
        "    contour_color=\"black\",  # This should also work now\n",
        "    color_func=white_color_func,\n",
        "    # mask=mask,\n",
        ").generate(word_string)\n",
        "\n",
        "# Generate and display the word cloud\n",
        "plt.figure(figsize=(16, 9))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Save the word cloud as an image\n",
        "wordcloud.to_file(\"wordcloud_berita2600_2023.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "2Ki7XJYm2m4C",
        "outputId": "f7b1507a-350d-4d4e-e1f6-ba41b804c13e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Charts"
      ],
      "metadata": {
        "id": "TJgPY-yxqVsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load DataFrame"
      ],
      "metadata": {
        "id": "Fb9iffaTqdZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access raw word frequencies from the generated word cloud\n",
        "word_counts = wordcloud.process_text(clean_text_string)\n",
        "\n",
        "# Create a DataFrame of word frequencies\n",
        "word_counts_df = pd.DataFrame.from_dict(word_counts, orient='index', columns=['freq'])\n",
        "\n",
        "# Sort by word count in descending order\n",
        "word_counts_df = word_counts_df.sort_values(by='freq', ascending=False)\n",
        "\n",
        "# Print the top 10 words and their frequencies\n",
        "n=10\n",
        "print(word_counts_df.head(n))\n",
        "\n",
        "# Optionally, save the DataFrame to a CSV file\n",
        "# word_counts_df.to_csv(f\"top_{n}_words.csv\", index=False)"
      ],
      "metadata": {
        "id": "BmbQfpR6jqLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9427481-f72e-44c5-fa28-b1261b78f57e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter  # Import Counter for word counting\n",
        "\n",
        "# Access the words and frequencies from the generated word cloud\n",
        "words_and_frequencies = wordcloud.words_\n",
        "\n",
        "# Count unique words, excluding stopwords\n",
        "unique_word_count = 0\n",
        "for word, frequency in words_and_frequencies.items():\n",
        "    if word not in stopwords_c:  # Check if not a stopword\n",
        "        unique_word_count += 1\n",
        "\n",
        "# Print the results\n",
        "print(\"Number of unique words (excluding stopwords):\", unique_word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZhZtWg0ZfOJ",
        "outputId": "c9ce86df-4977-4c9e-dd49-dab086bd749b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word_counts_df"
      ],
      "metadata": {
        "id": "9pyx7ufvCpsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_frequency = word_counts_df['freq'].sum()  # Sum all frequencies in the 'freq' column\n",
        "print(\"Total frequency of all words & phrases:\", total_frequency)\n",
        "print(\"Average number of all words & phrases per post:\", total_frequency/183)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1XHSW-cGHEr",
        "outputId": "11eaa5f3-c1ae-4e74-9aba-3ced09217711"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Horizontal Bar Chart\n",
        "\n"
      ],
      "metadata": {
        "id": "UdjkCt2tqjdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt2\n",
        "plt2.style.use('dark_background')\n",
        "\n",
        "# Get the top n most common words\n",
        "n=15\n",
        "top_n_words = word_counts_df.head(n)\n",
        "\n",
        "# Create horizontal bar chart with reversed y-axis\n",
        "plt2.figure(figsize=(16, 9))\n",
        "ax = top_n_words.plot(kind='barh', color='dodgerblue')  # Set bar color to blue\n",
        "ax.invert_yaxis()\n",
        "\n",
        "# Add data labels at the end of each bar\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_width()), (p.get_width() + 1.4, p.get_y() + p.get_height() / 1.2))\n",
        "\n",
        "# Customize the chart appearance\n",
        "# plt2.xlabel(\"Frequency\")\n",
        "# plt2.ylabel(\"Words/Phrases\")\n",
        "\n",
        "plt2.rcParams['font.size'] = 10  # Set overall font size for text elements\n",
        "plt2.title(f\"Top {n} Common Words & Phrases\", y=1.03, fontsize=16)\n",
        "# plt2.title(f\"Top {n} Common Words & Phrases\", y=1.03)\n",
        "# plt2.tight_layout()\n",
        "\n",
        "# Remove top and right lines\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "# Remove ticks from top and right axes\n",
        "ax.get_xaxis().tick_bottom()\n",
        "ax.get_yaxis().tick_left()\n",
        "\n",
        "# Show the chart\n",
        "plt2.draw()\n",
        "plt2.show()\n"
      ],
      "metadata": {
        "id": "Vkxjdevrq-hm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "996eb2f7-5b47-4654-cd1c-b5673aa3e8e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt2b\n",
        "\n",
        "plt2b.style.use('dark_background')\n",
        "\n",
        "# Get the top n most common words\n",
        "n=15\n",
        "top_n_words = word_counts_df.head(n)\n",
        "\n",
        "# Create vertical bar chart\n",
        "plt2b.figure(figsize=(9, 16))  # Adjust figure size for vertical bars\n",
        "ax = top_n_words.plot(kind='bar', color='dodgerblue')  # Create vertical bars\n",
        "\n",
        "# Remove x-axis labels\n",
        "# plt2b.xticks([])\n",
        "\n",
        "# Add data labels at the top of each bar\n",
        "# for p in ax.patches:\n",
        "#     ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height() + 5))\n",
        "\n",
        "# Customize the chart appearance\n",
        "# plt2b.xlabel(\"Word/Phrase\")\n",
        "# plt2b.ylabel(\"Frequency\")\n",
        "\n",
        "# plt2b.rcParams['font.size'] = 10\n",
        "plt2b.title(f\"Top {n} Common Words\", y=1.03, fontsize=16)\n",
        "\n",
        "# Remove top and right lines\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "# Remove ticks from top and right axes\n",
        "ax.get_xaxis().tick_bottom()\n",
        "ax.get_yaxis().tick_left()\n",
        "\n",
        "plt2b.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "pjYrAj5mxKGi",
        "outputId": "d3cf19a3-0a0d-4500-da68-56aef8cda045"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pie Chart"
      ],
      "metadata": {
        "id": "NK1aqEOHsxa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt3\n",
        "\n",
        "# Select top 15 words and aggregate others into \"others\" category\n",
        "n = 15\n",
        "top_n_words = word_counts_df.head(n)\n",
        "other_words = word_counts_df.tail(-15).sum()\n",
        "\n",
        "# Combine top 15 words into a single \"Top 15 Words\" category\n",
        "top_n_words = top_n_words.sum().rename(f\"Top {n} Words\")\n",
        "\n",
        "# Create a DataFrame with the two desired categories\n",
        "pie_data = pd.DataFrame({'freq': [top_n_words, other_words]}, index=[f'[Top {n} Words & Phrases]', '[Other Words & Phrases]'])\n",
        "\n",
        "# Create pie chart with blue and light gray colors\n",
        "colors = ['dodgerblue', 'dimgrey']  # Define the color palette\n",
        "plt3.figure(figsize=(6, 6))\n",
        "wedges, texts, autotexts = plt3.pie(pie_data['freq'], labels=pie_data.index, autopct=\"%1.1f%%\", colors=colors)\n",
        "\n",
        "# Increase font size for all elements\n",
        "plt3.rcParams['font.size'] = 10  # Set overall font size for text elements\n",
        "\n",
        "# Adjust title position and font size\n",
        "plt3.title(f\"Top {n} Words/Phrases Distribution\", fontsize=14)  # Move title closer and increase font size\n",
        "plt3.show()\n"
      ],
      "metadata": {
        "id": "-_XFsBveDrBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "06472061-cae9-4854-d693-343ef02e100d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}